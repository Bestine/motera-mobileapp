{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "779c3d74",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 0,
        "width": 12
       }
      }
     }
    }
   },
   "source": [
    "# Data Orchestrations\n",
    "Data orchestration is the process of managing and coordinating data across different systems, tools, and storage locations to ensure it flows smoothly and is ready for analysis. Think of it as a conductor in an orchestra, guiding various \"instruments\" (data sources) to work together in harmony, so the right data reaches the right place at the right time. This often involves automating tasks like data extraction, transformation, and loading (ETL) to keep everything running efficiently and consistently across an organization\n",
    "\n",
    "## Definitions of ETL and ELT\n",
    "- **ETL:** ETL is ideal when data needs to be standardized and cleaned before storage, often used in on-premise systems where resources for transformation are outside the target system.\n",
    "- **ELT:** ELT is preferred in modern, cloud-based architectures with scalable storage and processing power, where data can be quickly loaded and transformed in the target system-\n",
    "\n",
    "## Setbacks and Efficiency of each method\n",
    "### ETL \n",
    "#### Efficiency\n",
    "- **Processing Load:** Transformation happens outside the data warehouse, so it requires external processing resources, which can slow down the ETL process if resources are limited.\n",
    "- **Data Accuracy:** Since data is cleaned and standardized before it reaches the target, it often results in cleaner data being stored initially, reducing downstream processing needs.\n",
    "\n",
    "#### Drawbacks\n",
    "- **Scalability:** Scaling ETL can be challenging, especially with large data volumes, since transformation is limited by the external processing environment.\n",
    "- **Complexity:** ETL requires predefined transformations before loading, making it less flexible when requirements or data structures change.\n",
    "- **Latency:** ETL can introduce delays in data availability because transformations must complete before loading data into the target system.\n",
    "\n",
    "### ELT \n",
    "#### Efficiency\n",
    "- **Scalability:** ELT leverages the power of modern cloud data warehouses (like Snowflake, BigQuery), which are highly scalable and can process large volumes of data in parallel.\n",
    "- **Speed:** Since data is loaded directly into the target system, it’s available sooner, and transformations can be run on-demand or scheduled, reducing latency.\n",
    "- **Flexibility:** Raw data is stored as-is, allowing for diverse transformations directly in the warehouse, accommodating changing data needs.\n",
    "\n",
    "#### Drawbacks\n",
    "- **Data Storage Costs:** Loading raw data can increase storage costs, especially if much of the raw data is unused.\n",
    "- **Data Quality Risks:** Since transformations happen after loading, raw data might include duplicates, errors, or inconsistencies that need extra management within the warehouse.\n",
    "- **Performance:** Running transformations in the data warehouse could impact query performance, especially if transformations are complex or unoptimized.\n",
    "\n",
    "## Tools required \n",
    "- Extract data using Fivetran (for automated connectors) or Kafka (for real-time data).\n",
    "- Transform with dbt or Spark for scalable processing.\n",
    "- Store in a data warehouse like Snowflake.\n",
    "- Orchestrate using Apache Airflow to automate the ETL process.\n",
    "- Monitor with Grafana and set alerts on Slack.\n",
    "\n",
    "\n",
    "## Batch and Stream Processing \n",
    "- **Batch Processing:** High latency, large volumes, periodic processing (e.g., overnight reporting).\n",
    "- **Stream Processing:** Low latency, real-time, continuous processing (e.g., fraud detection, live monitoring)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46610f20",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "# Data Structures and Algorithms (DSA)\n",
    "## Big O -notation \n",
    "-\n",
    "-\n",
    "-\n",
    "\n",
    "## Basic data structures in Python \n",
    "- Lists\n",
    "- Tuple \n",
    "- Sets \n",
    "- Arrays\n",
    "\n",
    "## Sorting Algorithms \n",
    "- Bubble Sort \n",
    "- Selection Sort \n",
    "- Insertion Sort \n",
    "- Merge Sort \n",
    "- Quick Sort \n",
    "- Counting Sort \n",
    "- Radix Sort \n",
    "- Bucket Sort\n",
    "\n",
    "## Search Algorithms \n",
    "-\n",
    "-\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5937ad9",
   "metadata": {},
   "source": [
    "# Webscraping Frameworks \n",
    "Web scraping is a process of automatically extracting information from websites. Instead of copying data manually, a web scraper (a small program) goes through the website’s pages and pulls specific data, like prices, product details, or articles, into a structured format like a spreadsheet or database. It’s useful for tasks like tracking prices on e-commerce sites, gathering news articles, or analyzing social media trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d594f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Httpx and requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09085107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium and Playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "366abdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a985cd",
   "metadata": {},
   "source": [
    "# Database Queries \n",
    "- Find best interview questions on database queries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e308683",
   "metadata": {},
   "source": [
    "# Potential Improvement the company can implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc1623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "default_view",
    "version": 1,
    "views": {
     "default_view": {
      "cellMargin": 10,
      "defaultCellHeight": 40,
      "maxColumns": 12,
      "name": "active_view",
      "type": "grid"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
